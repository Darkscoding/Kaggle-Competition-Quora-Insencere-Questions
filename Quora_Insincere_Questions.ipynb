{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import operator\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing Notebook: Using Word Embeddings\n",
    "\n",
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  1306122\n",
      "Testing set size:  56370\n",
      "The total number of texts is:  1362492\n",
      "Proportion of Training set to Total Set:  95.86272800133872 %\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv').drop('target', axis=1) # Drop the 'Target' Feature/Column, so dimensions match when concatenating\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "df = pd.concat([train ,test])\n",
    "\n",
    "print('Training set size: ', train.shape[0])\n",
    "print('Testing set size: ', test.shape[0])\n",
    "print('The total number of texts is: ', df.shape[0])\n",
    "\n",
    "print('Proportion of Training set to Total Set: ', f\"{(train.shape[0])/(train.shape[0]+test.shape[0])*100} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00002165364db923c7e6</td>\n",
       "      <td>How did Quebec nationalists see their province...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000032939017120e6e44</td>\n",
       "      <td>Do you have an adopted dog, how would you enco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000412ca6e4628ce2cf</td>\n",
       "      <td>Why does velocity affect time? Does velocity a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000042bf85aa498cd78e</td>\n",
       "      <td>How did Otto von Guericke used the Magdeburg h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000455dfa3e01eae3af</td>\n",
       "      <td>Can I convert montra helicon D to a mountain b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid                                      question_text\n",
       "0  00002165364db923c7e6  How did Quebec nationalists see their province...\n",
       "1  000032939017120e6e44  Do you have an adopted dog, how would you enco...\n",
       "2  0000412ca6e4628ce2cf  Why does velocity affect time? Does velocity a...\n",
       "3  000042bf85aa498cd78e  How did Otto von Guericke used the Magdeburg h...\n",
       "4  0000455dfa3e01eae3af  Can I convert montra helicon D to a mountain b..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Vocabulary\n",
    "i.e. Python Dictionary containing the occurrences of words in our training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(sentences):\n",
    "    vocab = {}\n",
    "    for sentence in tqdm(sentences):\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except:\n",
    "                vocab[word] = 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1306122/1306122 [00:06<00:00, 215820.98it/s]\n",
      "100%|██████████| 1306122/1306122 [00:04<00:00, 310036.62it/s]\n"
     ]
    }
   ],
   "source": [
    "questions = train['question_text'].progress_apply(lambda x: x.split()).values\n",
    "vocab = create_vocabulary(questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's verify that it worked by checking the count of the first 5 elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'How': 261930, 'did': 33489, 'Quebec': 97, 'nationalists': 91, 'see': 9003}\n"
     ]
    }
   ],
   "source": [
    "print({k: vocab[k] for k in list(vocab)[:5]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for importing embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embed(file):\n",
    "    def get_coefs(word,*arr): \n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "    \n",
    "    if file == '../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec':\n",
    "        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file) if len(o)>100)\n",
    "    else:\n",
    "        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file, encoding='latin'))\n",
    "        \n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_path = 'embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "google_news_path ='embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin' \n",
    "wiki_news_path = 'embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
    "paragram_path = 'embeddings/paragram_300_sl999/paragram_300_sl999.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_google = KeyedVectors.load_word2vec_format(google_news_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embed glove loaded.\n",
      "Embed wiki loaded.\n",
      "Embed paragram loaded.\n"
     ]
    }
   ],
   "source": [
    "print('Embed glove loaded.')\n",
    "embed_glove = load_embed(glove_path)\n",
    "#embed_google = load_embed(google_news_path)\n",
    "print('Embed wiki loaded.')\n",
    "embed_wiki = load_embed(wiki_news_path)\n",
    "print('Embed paragram loaded.')\n",
    "embed_paragram = load_embed(paragram_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_coverage(embedding, vocab):\n",
    "    known_words = {}\n",
    "    unknown_words = {}\n",
    "    number_known_words = 0\n",
    "    number_unknown_words = 0\n",
    "    if embedding == embed_google:\n",
    "        for word in tqdm(vocab):\n",
    "            try:\n",
    "                known_words[word] = embedding[word]\n",
    "                number_known_words += vocab[word]\n",
    "            except:\n",
    "                unknown_words[word] = vocab[word]\n",
    "                number_unknown_words += vocab[word]\n",
    "        print(f\"Percentage of embeddings for Vocab is: {(len(known_words)/len(vocab))*100}%\")\n",
    "        print(f\"Percentage of embeddings for Text is: {(number_known_words/(number_known_words + number_unknown_words))*100}%\")\n",
    "    else:\n",
    "        for word in tqdm(vocab.keys()):\n",
    "            try:\n",
    "                known_words[word] = embedding[word]\n",
    "                number_known_words += vocab[word]\n",
    "            except:\n",
    "                unknown_words[word] = vocab[word]\n",
    "                number_unknown_words += vocab[word]\n",
    "        print(f\"Percentage of embeddings for Vocab is: {(len(known_words)/len(vocab))*100}%\")\n",
    "        print(f\"Percentage of embeddings for Text is: {(number_known_words/(number_known_words + number_unknown_words))*100}%\")\n",
    "\n",
    "    unknown_words = sorted(unknown_words.items(), key=operator.itemgetter(1))[::-1]\n",
    "    \n",
    "    return unknown_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 114/508823 [00:00<07:30, 1130.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking coverage for Google.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 508823/508823 [00:29<00:00, 17234.34it/s]\n",
      "  0%|          | 162/508823 [00:00<05:16, 1607.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of embeddings for Vocab is: 24.308453037696804%\n",
      "Percentage of embeddings for Text is: 78.74644592896665%\n",
      "Checking coverage for Glove.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 508823/508823 [00:21<00:00, 23290.55it/s]\n",
      "  0%|          | 0/508823 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of embeddings for Vocab is: 33.0242540136747%\n",
      "Percentage of embeddings for Text is: 88.14782041294316%\n",
      "Checking coverage for Wiki.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 508823/508823 [00:08<00:00, 58627.95it/s] \n",
      "  0%|          | 261/508823 [00:00<03:16, 2594.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of embeddings for Vocab is: 29.863233383711034%\n",
      "Percentage of embeddings for Text is: 87.64399563812303%\n",
      "Checking coverage for Paragram.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 508823/508823 [00:12<00:00, 40295.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of embeddings for Vocab is: 19.54097986922761%\n",
      "Percentage of embeddings for Text is: 72.20571143729778%\n"
     ]
    }
   ],
   "source": [
    "print('Checking coverage for Google.')\n",
    "unknown_words_google = check_coverage(embed_google, vocab)\n",
    "print('Checking coverage for Glove.')\n",
    "unknown_words_glove = check_coverage(embed_glove, vocab)\n",
    "print('Checking coverage for Wiki.')\n",
    "unknown_words_wiki = check_coverage(embed_wiki, vocab)\n",
    "print('Checking coverage for Paragram.')\n",
    "unknown_words_para = check_coverage(embed_paragram, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The best results are obtained using the Glove embedding, however notice that approximately only 30% of our Vocab has an embedding. In other words, a bit over 10% of our data (100% - 88.147%) is useless. Let's see if we can fix that by investigating the 'unknown_words'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('India?', 16384),\n",
       " ('it?', 12900),\n",
       " (\"What's\", 12425),\n",
       " ('do?', 8753),\n",
       " ('life?', 7753),\n",
       " ('you?', 6295),\n",
       " ('me?', 6202),\n",
       " ('them?', 6140),\n",
       " ('time?', 5716),\n",
       " ('world?', 5386),\n",
       " ('people?', 4971),\n",
       " ('why?', 4943),\n",
       " ('Quora?', 4655),\n",
       " ('like?', 4487),\n",
       " ('for?', 4450),\n",
       " ('work?', 4206),\n",
       " ('2017?', 4050),\n",
       " ('mean?', 3971),\n",
       " ('2018?', 3594),\n",
       " ('country?', 3422)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknown_words_glove[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It seems like the punctuation marks are posing a problem for our embedding model. Let's take care of that.\n",
    "### Notice that we can include punctuation marks, since they are handled by Glove embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'?' in embed_glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'!' in embed_glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'#' in embed_glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'@' in embed_glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'$' in embed_glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n",
    "\n",
    "def check_punctuation(embedding, punct):\n",
    "    unknown_punct = ''\n",
    "    known_punct = ''\n",
    "    for p in punct:\n",
    "        if p in embedding:\n",
    "            known_punct += f'{p} '\n",
    "        else:\n",
    "            unknown_punct += f'{p} '\n",
    "    return known_punct, unknown_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_punct, unknown_punct = check_punctuation(embed_glove, punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Known symbols: / - ' ? ! . , # $ % ' ( ) * + - / : ; < = > @ [ \\ ] ^ _ ` { | } ~ \" \" \\ & \n",
      "Unknown symbols: “ ” ’ ∞ θ ÷ α • à − β ∅ ³ π ‘ ₹ ´ ° £ € × ™ √ ² — – \n"
     ]
    }
   ],
   "source": [
    "print(f'Known symbols: {known_punct}')\n",
    "print(f'Unknown symbols: {unknown_punct}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notice that several punctuation marks have 'semantic' synonyms that we could use in order to avoid losing valuable information in our representation. For example, € is typically used to denote a monetary amount, so we could replace occurences of this symbol with \\\\$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol_mapping = {\"‘\": \"'\", \"₹\": \"$\", \"´\": \"'\", \"°\": \"degrees\", \"€\": \"$\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\", \"`\": \"'\", '“': '\"', '”': '\"', '“': '\"', \"£\": \"$\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', 'β': 'beta', '∅': 'empty', '³': '3', 'π': 'pi', }\n",
    "\n",
    "def handle_punctuation(text, symbol_mapping, punct):\n",
    "    \n",
    "    for symbol in symbol_mapping:\n",
    "        text = text.replace(symbol, symbol_mapping[symbol])\n",
    "    \n",
    "    for p in punct:\n",
    "        text = text.replace(p, f' {p} ')\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1306122/1306122 [00:28<00:00, 46123.93it/s]\n"
     ]
    }
   ],
   "source": [
    "train[\"question_text\"] = train[\"question_text\"].progress_apply(lambda x: handle_punctuation(x, symbol_mapping, punct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00002165364db923c7e6</td>\n",
       "      <td>How did Quebec nationalists see their province...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000032939017120e6e44</td>\n",
       "      <td>Do you have an adopted dog ,  how would you en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000412ca6e4628ce2cf</td>\n",
       "      <td>Why does velocity affect time ?  Does velocity...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000042bf85aa498cd78e</td>\n",
       "      <td>How did Otto von Guericke used the Magdeburg h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000455dfa3e01eae3af</td>\n",
       "      <td>Can I convert montra helicon D to a mountain b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid                                      question_text\n",
       "0  00002165364db923c7e6  How did Quebec nationalists see their province...\n",
       "1  000032939017120e6e44  Do you have an adopted dog ,  how would you en...\n",
       "2  0000412ca6e4628ce2cf  Why does velocity affect time ?  Does velocity...\n",
       "3  000042bf85aa498cd78e  How did Otto von Guericke used the Magdeburg h...\n",
       "4  0000455dfa3e01eae3af  Can I convert montra helicon D to a mountain b..."
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1306122/1306122 [00:21<00:00, 60590.83it/s] \n",
      "100%|██████████| 1306122/1306122 [00:04<00:00, 303912.29it/s]\n"
     ]
    }
   ],
   "source": [
    "questions = train[\"question_text\"].progress_apply(lambda x: x.split()).values\n",
    "vocab = create_vocabulary(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 240550/240550 [00:04<00:00, 52967.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of embeddings for Vocab is: 74.15547703180212%\n",
      "Percentage of embeddings for Text is: 99.56469519430303%\n"
     ]
    }
   ],
   "source": [
    "unknown_words_glove = check_coverage(embed_glove, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Major improvements! From 33% of embeddings for our Vocab to 74%, and from 88% of embeddings for all of our Text to 99.5%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62169"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unknown_words_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Quorans', 856),\n",
       " ('Brexit', 493),\n",
       " ('cryptocurrencies', 481),\n",
       " ('Redmi', 379),\n",
       " ('…', 267),\n",
       " ('OnePlus', 125),\n",
       " ('UCEED', 123),\n",
       " ('Blockchain', 112),\n",
       " ('GDPR', 107),\n",
       " ('demonetisation', 106),\n",
       " ('Pokémon', 106),\n",
       " ('Coinbase', 105),\n",
       " ('BNBR', 99),\n",
       " ('Adityanath', 99),\n",
       " ('Machedo', 99),\n",
       " ('Boruto', 93),\n",
       " ('ethereum', 89),\n",
       " ('DCEU', 89),\n",
       " ('IIEST', 85),\n",
       " ('Qoura', 79),\n",
       " ('SJWs', 79),\n",
       " ('fiancé', 70),\n",
       " ('Upwork', 70),\n",
       " ('LNMIIT', 67),\n",
       " ('Zerodha', 65),\n",
       " ('Kavalireddi', 65),\n",
       " ('etc…', 63),\n",
       " ('bhakts', 63),\n",
       " ('Doklam', 62),\n",
       " ('NICMAR', 59),\n",
       " ('Vajiram', 59),\n",
       " ('Unacademy', 58),\n",
       " ('MUOET', 56),\n",
       " ('chsl', 55),\n",
       " ('HackerRank', 52),\n",
       " ('AlShamsi', 52),\n",
       " ('Bhakts', 51),\n",
       " ('Litecoin', 48),\n",
       " ('Awdhesh', 48),\n",
       " ('Cryptocurrency', 47),\n",
       " ('Jiren', 47),\n",
       " ('eLitmus', 47),\n",
       " ('altcoin', 45),\n",
       " ('altcoins', 45),\n",
       " ('Ryzen', 45),\n",
       " ('coinbase', 44),\n",
       " ('Baahubali', 44),\n",
       " ('SRMJEE', 43),\n",
       " ('Beerus', 41),\n",
       " ('Skripal', 40),\n",
       " ('SGSITS', 40),\n",
       " ('bahubali', 38),\n",
       " ('Binance', 37),\n",
       " ('Zebpay', 37),\n",
       " ('BMSCE', 37),\n",
       " ('What\\u200b', 36),\n",
       " ('Ravula', 36),\n",
       " ('Alshamsi', 36),\n",
       " ('Gurugram', 36),\n",
       " ('josaa', 35),\n",
       " ('SRMJEEE', 35),\n",
       " ('Golang', 35),\n",
       " ('upwork', 35),\n",
       " ('BIPC', 35),\n",
       " ('litecoin', 34),\n",
       " ('Swachh', 34),\n",
       " ('Truecaller', 33),\n",
       " ('MHCET', 33),\n",
       " ('OBOR', 33),\n",
       " ('clickbait', 32),\n",
       " ('Chromecast', 30),\n",
       " ('iisc', 30),\n",
       " ('TensorFlow', 30),\n",
       " ('USICT', 29),\n",
       " ('PESSAT', 29),\n",
       " ('Adhaar', 29),\n",
       " ('adhaar', 29),\n",
       " ('Patreon', 28),\n",
       " ('Koinex', 28),\n",
       " ('Diestm', 28),\n",
       " ('demonitisation', 28),\n",
       " ('unacademy', 27),\n",
       " ('Whst', 27),\n",
       " ('microservices', 27),\n",
       " ('hyperloop', 26),\n",
       " ('IISERs', 26),\n",
       " ('UPSE', 25),\n",
       " ('ReactJS', 25),\n",
       " ('Bittrex', 25),\n",
       " ('Byju', 25),\n",
       " ('fortnite', 25),\n",
       " ('Demonetization', 25),\n",
       " ('FTRE', 24),\n",
       " ('LBSNAA', 24),\n",
       " ('Irodov', 24),\n",
       " ('Zamasu', 24),\n",
       " ('Trumpcare', 24),\n",
       " ('brexit', 24),\n",
       " ('Tensorflow', 24),\n",
       " ('JoSAA', 23)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknown_words_glove[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Pokemon' in embed_glove, 'Hackerrank' in embed_glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle accentuations (Pokémon), acronyms (GDPR), mispellings, capital letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_upper_lower(embed, vocab):\n",
    "    count_lower = 0\n",
    "    count_upper = 0\n",
    "    for word in vocab:\n",
    "        if word in embed and word.lower() not in embed:\n",
    "            embed[word.lower()] = embed[word]\n",
    "            count_lower += 1\n",
    "        elif word.lower() in embed and word not in embed:\n",
    "            embed[word] = embed[word.lower()]\n",
    "            count_upper += 1\n",
    "    print(f'Added {count_lower} lowercase words to the embedding.')\n",
    "    print(f'Added {count_upper} uppercase words to the embedding.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 17625 lowercase words to the embedding.\n",
      "Added 2334 uppercase words to the embedding.\n"
     ]
    }
   ],
   "source": [
    "add_upper_lower(embed_glove, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 240550/240550 [00:00<00:00, 673183.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of embeddings for Vocab is: 75.83205154853461%\n",
      "Percentage of embeddings for Text is: 99.59876270684337%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "unknown_words_glove = check_coverage(embed_glove, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Quorans', 856),\n",
       " ('Brexit', 493),\n",
       " ('cryptocurrencies', 481),\n",
       " ('Redmi', 379),\n",
       " ('…', 267),\n",
       " ('OnePlus', 125),\n",
       " ('UCEED', 123),\n",
       " ('GDPR', 107),\n",
       " ('demonetisation', 106),\n",
       " ('Pokémon', 106)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknown_words_glove[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonym_mapping = {'cryptocurrencies': 'cryptocurrency', 'qoura': 'quora', 'Pokémon': 'pokemon',  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "mispell_dict = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'demonetization' in embed_glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prec():\n",
    "    ## fill up the missing values\n",
    "    train_X = train_df[\"question_text\"].fillna(\"_##_\").values\n",
    "    test_X = test_df[\"question_text\"].fillna(\"_##_\").values\n",
    "\n",
    "    ## Tokenize the sentences\n",
    "    tokenizer = Tokenizer(num_words=max_features)\n",
    "    tokenizer.fit_on_texts(list(train_X))\n",
    "    train_X = tokenizer.texts_to_sequences(train_X)\n",
    "    test_X = tokenizer.texts_to_sequences(test_X)\n",
    "\n",
    "    ## Pad the sentences \n",
    "    train_X = pad_sequences(train_X, maxlen=maxlen)\n",
    "    test_X = pad_sequences(test_X, maxlen=maxlen)\n",
    "\n",
    "    ## Get the target values\n",
    "    train_y = train_df['target'].values\n",
    "    \n",
    "    #shuffling the data\n",
    "    np.random.seed(2018)\n",
    "    trn_idx = np.random.permutation(len(train_X))\n",
    "\n",
    "    train_X = train_X[trn_idx]\n",
    "    train_y = train_y[trn_idx]\n",
    "    \n",
    "    return train_X, test_X, train_y, tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Scipy Stacks\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split # To split our training set into training/validation sets.\n",
    "from sklearn import metrics\n",
    "\n",
    "# We are performing a sequential neural network.\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Used to process text data.\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# What layers will be involved with our neural network.\n",
    "from keras.layers import Dense, LSTM, Embedding, Dropout, Activation, GRU, LSTM, Conv1D, MaxPooling1D, Bidirectional, GlobalMaxPool1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
